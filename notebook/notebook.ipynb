{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Etx1q5Ksts7F"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from random_predictor import RandomPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from  sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple, Dict, Any\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8-white\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = int(os.getenv(\"RANDOM_STATE\"))\n",
    "DATA_SOURCE_URL = str(os.getenv(\"DATA_SOURCE_URL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE7z9e7Gtv6s"
   },
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJtY3OAufoGH"
   },
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "  \"sepal_length\", \n",
    "  \"sepal_width\", \n",
    "  \"petal_length\", \n",
    "  \"petal_width\", \n",
    "  \"species\"\n",
    "]\n",
    "\n",
    "raw_data = pd.read_csv(\n",
    "  DATA_SOURCE_URL, \n",
    "  names=col_names\n",
    ")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fx07MktgBT_"
   },
   "source": [
    "## Assesing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset information\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing value(s) if any\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate(s) data if any\n",
    "raw_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview duplicates data\n",
    "raw_data[raw_data.duplicated(keep=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataframe to create new dataframe for cleaning purpose \n",
    "# and keep original raw data \n",
    "data = raw_data.copy()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous steps, the problem happend only duplicates data,\n",
    "# so in the cleaning steps, we only remove them to keep data clean.\n",
    "data.drop_duplicates(keep=\"first\", inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert \"class\" to proper datatype\n",
    "data[\"species\"] = data[\"species\"].astype(\"category\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test\n",
    "# we specify the 'stratify' parameter to keep both sets balance\n",
    "# and specify 'random_state' parameter to keep \n",
    "# the separation same every run of this cell\n",
    "train_set, test_set = train_test_split(\n",
    "                        data,\n",
    "                        test_size=.2,\n",
    "                        stratify=data[\"species\"],\n",
    "                        random_state=RANDOM_STATE\n",
    "                      )\n",
    "\n",
    "# preview data size\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick preview data after splitting\n",
    "display(train_set.sample(5))\n",
    "display(test_set.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYZUSglMgd4O"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the original class proportion \n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "data[\"species\"]\\\n",
    "  .value_counts()\\\n",
    "  .sort_index()\\\n",
    "  .plot(\n",
    "    kind=\"barh\", \n",
    "    title=\"Original Class Count\",\n",
    "    xlabel=\"count\"\n",
    "  );\n",
    "plt.subplot(122)\n",
    "data[\"species\"]\\\n",
    "  .value_counts(normalize=True)\\\n",
    "  .sort_index()\\\n",
    "  .plot(\n",
    "    kind=\"barh\", \n",
    "    title=\"Original Class Proportion\",\n",
    "    xlabel=\"percentage (%)\"\n",
    "  );\n",
    "plt.show();\n",
    "\n",
    "# Check the class proportion (should be balance after splitting)\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "train_set[\"species\"]\\\n",
    "  .value_counts(normalize=True)\\\n",
    "  .sort_index()\\\n",
    "  .plot(\n",
    "    kind=\"barh\", \n",
    "    title=\"Class Proportion in Train Set\",\n",
    "    xlabel=\"percentage (%)\"\n",
    "  );\n",
    "\n",
    "plt.subplot(122)\n",
    "test_set[\"species\"]\\\n",
    "  .value_counts(normalize=True)\\\n",
    "  .sort_index()\\\n",
    "  .plot(\n",
    "    kind=\"barh\", \n",
    "    title=\"Class Proportion in Test Set\",\n",
    "    xlabel=\"percentage (%)\"\n",
    "  );\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data distributions\n",
    "train_set.hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outliers\n",
    "train_set.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints  \n",
    "\n",
    "- Should we remove the outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "train_set.assign(species=train_set['species'].cat.codes).corr().style.background_gradient(\"bwr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation in visualization for better understanding\n",
    "sns.pairplot(data=train_set, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature / X and class / target / y\n",
    "features = list(data.columns[:-1])\n",
    "target = [data.columns[-1]]\n",
    "engineered_features = {\n",
    "    \"sepal_size\": [\"sepal_length\", \"sepal_width\"],\n",
    "    \"petal_size\": [\"petal_length\", \"petal_width\"]\n",
    "}\n",
    "\n",
    "df_fe = train_set.copy()\n",
    "for feature in list(engineered_features.keys()):\n",
    "    df_fe[feature] = train_set[engineered_features[feature]].prod(axis=1)\n",
    "    df_fe.drop(columns=engineered_features[feature], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_fe, x=\"sepal_size\", y=\"petal_size\", hue=\"species\");\n",
    "plt.title(\"New Features from Feature Engineering\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "df_fe.assign(species=df_fe['species'].cat.codes).corr().style.background_gradient(\"bwr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_fe, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cases:\n",
    "- Case 1: all features + scaled\n",
    "- Case 2: engineered features + normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature / X and class / target / y\n",
    "X = train_set[features] #data[features]\n",
    "y = train_set[target] # data[target]\n",
    "\n",
    "X_fe = pd.DataFrame()\n",
    "for feature in list(engineered_features.keys()):\n",
    "    X_fe[feature] = X[engineered_features[feature]].prod(axis=1)\n",
    "\n",
    "display(X.head(5))\n",
    "display(X_fe.head(5))\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "normalizer = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "scaler.fit(X)\n",
    "normalizer.fit(X_fe)\n",
    "encoder.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X)\n",
    "display(X_train[:5])\n",
    "\n",
    "X_train_fe = normalizer.transform(X_fe)\n",
    "display(X_train_fe[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(data=pd.concat([X_fe, y], axis=1), x=\"sepal_size\", y=\"petal_size\", hue=\"species\")\n",
    "plt.title(\"New Features from Feature Engineering\")\n",
    "plt.axvline(0, c='k', linestyle=\"--\", linewidth=.5)\n",
    "plt.axhline(0, c='k', linestyle=\"--\", linewidth=.5)\n",
    "\n",
    "\n",
    "tmp = pd.DataFrame(X_train_fe, columns=normalizer.feature_names_in_)\n",
    "tmp['species'] = y.values\n",
    "tmp.sort_values(by=\"species\", inplace=True)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.scatterplot(data=tmp, x=\"sepal_size\", y=\"petal_size\", hue=\"species\")\n",
    "plt.title(\"New Features from Feature Engineering (Normalized)\")\n",
    "plt.axvline(0, c='k', linestyle=\"--\", linewidth=.5)\n",
    "plt.axhline(0, c='k', linestyle=\"--\", linewidth=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(X_train, columns=scaler.feature_names_in_)\n",
    "tmp['species'] = y.values\n",
    "tmp.sort_values(by=\"species\", inplace=True)\n",
    "tmp[tmp.columns[[0,2,3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_x = 1\n",
    "idx_y = 3\n",
    "\n",
    "tmp = X.copy()\n",
    "tmp['species'] = y.values\n",
    "tmp.sort_values(by=\"species\", inplace=True)\n",
    "tmp = tmp[tmp.columns[[idx_x, idx_y, 4]]]\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(data=tmp, x=tmp.columns[0], y=tmp.columns[1], hue=\"species\")\n",
    "plt.title(\"Original Sepal\")\n",
    "plt.axvline(0, c='k', linestyle=\"--\", linewidth=.5)\n",
    "plt.axhline(0, c='k', linestyle=\"--\", linewidth=.5)\n",
    "\n",
    "tmp = pd.DataFrame(X_train, columns=scaler.feature_names_in_)\n",
    "tmp['species'] = y.values\n",
    "tmp.sort_values(by=\"species\", inplace=True)\n",
    "tmp = tmp[tmp.columns[[idx_x, idx_y, 4]]]\n",
    "plt.subplot(122)\n",
    "sns.scatterplot(data=tmp, x=tmp.columns[0], y=tmp.columns[1], hue=\"species\")\n",
    "plt.title(\"Scaled Sepal\")\n",
    "plt.axvline(0, c='k', linestyle=\"--\", linewidth=.5)\n",
    "plt.axhline(0, c='k', linestyle=\"--\", linewidth=.5)\n",
    "plt.axvline(1, c='k', linestyle=\"--\", linewidth=.5)\n",
    "plt.axhline(1, c='k', linestyle=\"--\", linewidth=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = encoder.transform(y)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_ZMmraatyS9"
   },
   "source": [
    "# Build Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several input data cases and models will be used:\n",
    "1. Random model, as the baseline model\n",
    "2. Logistic Regression with input data of all normalized features as model 1a\n",
    "3. SVM with input data of all normalized features as model 2a\n",
    "3. Logistic Regression with input data of standardized engineering features as model 1b\n",
    "4. SVM with standardized engineering features as model 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "rnd = RandomPredictor(np.unique(y_train))\n",
    "lr = LogisticRegression(random_state=RANDOM_STATE, multi_class=\"multinomial\")\n",
    "lr_fe = LogisticRegression(random_state=RANDOM_STATE, multi_class=\"multinomial\")\n",
    "svm = SVC(random_state=RANDOM_STATE, kernel='linear', probability=True)\n",
    "svm_fe = SVC(random_state=RANDOM_STATE, kernel='linear', probability=True)\n",
    "\n",
    "# fit/train models\n",
    "lr.fit(X_train, y_train)\n",
    "lr_fe.fit(X_train_fe, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_fe.fit(X_train_fe, y_train)\n",
    "\n",
    "# collect models into one variable for ease in later processes\n",
    "trained_models = {\n",
    "    \"Random\": rnd,\n",
    "    \"LogisticRegression\": lr,\n",
    "    \"SVM\": svm,\n",
    "    \"LogisticRegression_FE\": lr_fe,\n",
    "    \"SVM_FE\": svm_fe,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zDvJ0rGt5dB"
   },
   "source": [
    "# Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_set[target]\n",
    "display(y_test)\n",
    "y_test = encoder.transform(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform((test_set[features]))\n",
    "display(X_test[:5])\n",
    "\n",
    "X_test_fe = pd.DataFrame()\n",
    "for feature in list(engineered_features.keys()):\n",
    "    X_test_fe[feature] = test_set[engineered_features[feature]].prod(axis=1)\n",
    "display(X_test_fe[:5])\n",
    "X_test_fe = normalizer.transform(X_test_fe)\n",
    "display(X_test_fe[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(trained_models):\n",
    "  used_model = trained_models[model]\n",
    "  test_input = X_test if \"FE\" not in model else X_test_fe\n",
    "  prediction_result = used_model.predict(test_input)\n",
    "  header = f\" {i+1}. Test Result from: '{model}' \"\n",
    "  print(f\"{header:=^55s}\")\n",
    "  print(classification_report(y_test, prediction_result))\n",
    "  print(confusion_matrix(y_test, prediction_result))\n",
    "  print(\"=\"*55, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsGBoQG4t7vw"
   },
   "source": [
    "# Test Use Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results show that the **SVM** model excels in both input data cases; therefore, let's try to do a prediction simulation here before creating an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check species / class mapping\n",
    "for iris in encoder.classes_:\n",
    "  print(iris, \"->\", encoder.transform([iris]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olKg_dE_t1z7"
   },
   "outputs": [],
   "source": [
    "# Simulate input prediction\n",
    "test_case = test_set.sample() \n",
    "test_case = test_set.loc[10].to_frame().T \n",
    "display(test_case)\n",
    "\n",
    "test_case_input = test_case.drop(columns=[\"species\"]) \n",
    "display(test_case_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make prediction from one data point\n",
    "SVM = SVC()\n",
    "SVM.fit(X_train, y_train)\n",
    "SVM.predict(test_case_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints  \n",
    "\n",
    "- Is this prediction method correct?\n",
    "- Why is the prediction result different from the evaluation? (in the evaluation, no mistakes in prediction for either the SVM or logistic regression model for the 'Iris-setosa' species)\n",
    "- Does the model used to perform inference reflect the trained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make prediction with trained models.\n",
    "# NOTE: the correct prediction should output [0], the Iris-setosa species\n",
    "\n",
    "# predict one data point (sample)\n",
    "for model in trained_models.keys():\n",
    "    model_prediction = trained_models[model].predict(test_case_input)\n",
    "    print(f\"{model} prediction: {model_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints  \n",
    "\n",
    "- How to fix errors?\n",
    "- How do we make predictions more meaningful to users?\n",
    "- Why are the prediction results not correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interprete class / decode for better interpretation \n",
    "for i in range(3):\n",
    "  # YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do data processing same as the dev. process before pass to the model\n",
    "test_case_input_normalized = # YOUR CODE HERE (transform data) \n",
    "display(test_case_input_normalized)\n",
    "\n",
    "# predict one data point (sample)\n",
    "for model in list(trained_models.keys())[:-2]:\n",
    "    model_prediction = trained_models[model].predict(test_case_input_normalized)\n",
    "    iris_species_prediction = encoder.inverse_transform(model_prediction)[0]\n",
    "    prediction_proba = trained_models[model].predict_proba(test_case_input) * 100 # add probability as a confidence of prediction\n",
    "    print(f\"{model} prediction: {prediction_proba.max():.2f}% is {model_prediction} / {iris_species_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do data processing with feature engineering same as the dev. process before pass to the model\n",
    "test_case_input_fe = pd.DataFrame() \n",
    "for feature in list(engineered_features.keys()):\n",
    "    test_case_input_fe[feature] = test_case_input[engineered_features[feature]].prod(axis=1)\n",
    "display(test_case_input_fe)\n",
    "\n",
    "# predict one data point (sample)\n",
    "test_case_input_fe_scaled = # YOUR CODE HERE (transform data) \n",
    "for model in list(trained_models.keys())[-2:]:\n",
    "    model_prediction = trained_models[model].predict(test_case_input_fe_scaled)\n",
    "    iris_species_prediction = encoder.inverse_transform(model_prediction)[0]\n",
    "    prediction_proba = trained_models[model].predict_proba(test_case_input_fe_scaled) * 100\n",
    "    print(f\"{model} prediction: {prediction_proba.max():.2f}% is {model_prediction} / {iris_species_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints   \n",
    "\n",
    "- If the model is to be used in API/Web, what kind of input can the user provide? Can the user directly input the dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save objects\n",
    "saved_object_path = \"../utils\"\n",
    "if \"Random\" in trained_models.keys(): del trained_models[\"Random\"]\n",
    "\n",
    "all_features = {\n",
    "    \"original\": features,\n",
    "    \"engineered\": engineered_features\n",
    "}\n",
    "\n",
    "joblib.dump(trained_models, f'{saved_object_path}/models.bin')\n",
    "joblib.dump(df_fe, f'{saved_object_path}/data_feature_engineering.bin')\n",
    "\n",
    "# save encoder, normalizer, scaler, and all_features\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample json input as a simulation input from user\n",
    "raw_user_input = {\n",
    "  \"sepal_width\": 3.7,\n",
    "  \"petal_width\": 0.2,\n",
    "  \"petal_length\": 1.5,\n",
    "  \"sepal_length\": 5.4\n",
    "}\n",
    "raw_user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_case_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_objects() -> Tuple[\n",
    "        StandardScaler, \n",
    "        MinMaxScaler, \n",
    "        LabelEncoder, \n",
    "        Dict[str, Any], \n",
    "        Dict[str, object]\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Load pre-saved machine learning objects including scaler, normalizer, encoder,\n",
    "    feature engineering dictionary, and models.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[StandardScaler, Normalizer, LabelEncoder, Dict[str, Any], Dict[str, object]]:\n",
    "        - loaded_scaler: Scaler used to scale input data.\n",
    "        - loaded_normalizer: Normalizer used to normalize input data.\n",
    "        - loaded_encoder: Encoder used to decode model predictions.\n",
    "        - loaded_features: Dictionary containing feature engineering instructions.\n",
    "        - loaded_models: Dictionary of trained machine learning models.\n",
    "    \"\"\"\n",
    "\n",
    "    loaded_scaler = joblib.load(f'{saved_object_path}/scaler.bin') \n",
    "    loaded_normalizer = joblib.load(f'{saved_object_path}/normalizer.bin') \n",
    "    loaded_encoder = joblib.load(f'{saved_object_path}/encoder.bin') \n",
    "    loaded_features = joblib.load(f'{saved_object_path}/features.bin') \n",
    "    loaded_models = joblib.load(f'{saved_object_path}/models.bin') \n",
    "\n",
    "    return loaded_scaler, loaded_normalizer, loaded_encoder, loaded_features, loaded_models\n",
    "\n",
    "def data_pipeline_v1(\n",
    "        raw_input_df: pd.DataFrame, \n",
    "        scaler: MinMaxScaler\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess raw input data by normalizing it to be compatible with \n",
    "    model input requirements.\n",
    "\n",
    "    Args:\n",
    "        raw_input_df (pd.DataFrame): The raw input data in DataFrame format.\n",
    "        scaler (MinMaxScaler): Pre-trained scaler to normalize the data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Normalized input data ready for model consumption.\n",
    "    \"\"\"\n",
    "    # NOTE: Make sure the input is correct and feature names is correct\n",
    "    # display(raw_input_df)\n",
    "\n",
    "    # rearrange dataframe columns to fit normalizer\n",
    "    rearranged_features = list(scaler.feature_names_in_)\n",
    "    # display(rearranged_features)\n",
    "\n",
    "    raw_input_df = raw_input_df[rearranged_features]\n",
    "\n",
    "    # normalize data\n",
    "    input_df = scaler.transform(raw_input_df)\n",
    "\n",
    "    return input_df\n",
    "\n",
    "def data_pipeline_v2(\n",
    "        raw_input_df: pd.DataFrame, \n",
    "        features: Dict[str, Any], \n",
    "        normalizer: StandardScaler\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess raw input data by applying feature engineering and scaling \n",
    "    to prepare it for model input.\n",
    "\n",
    "    Args:\n",
    "        raw_input_df (pd.DataFrame): The raw input data in DataFrame format.\n",
    "        features (dict): Dictionary containing feature engineering instructions.\n",
    "        normalizer (StandardScaler): Pre-trained scaler to scale the data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Scaled input data ready for model consumption.\n",
    "    \"\"\"\n",
    "    # NOTE: Make sure the input is correct and feature names is correct\n",
    "    # do feature engineering\n",
    "    for feature in list(features.keys()):\n",
    "        raw_input_df[feature] = raw_input_df[features[feature]].prod(axis=1)\n",
    "\n",
    "    # rearrange dataframe columns to fit scaler\n",
    "    rearranged_features = list(normalizer.feature_names_in_)\n",
    "    raw_input_df = raw_input_df[rearranged_features]\n",
    "\n",
    "    # scale data\n",
    "    input_df = normalizer.transform(raw_input_df)\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def prediction_pipeline(\n",
    "        raw_input_from_user: dict, \n",
    "        model_name: str\n",
    "    ) -> Tuple[np.ndarray, float, str]:\n",
    "    \"\"\"\n",
    "    Run the prediction pipeline, processing raw user input and making a prediction using the specified model.\n",
    "\n",
    "    Args:\n",
    "        raw_input_from_user (dict): Dictionary of user inputs formatted for model compatibility.\n",
    "        model_name (str): The name of the model to use for prediction.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float, str]: \n",
    "        - prediction: The raw prediction output from the model.\n",
    "        - prediction_proba: The prediction probability from the model.\n",
    "        - prediction_str: The human-readable string output decoded from the model prediction.\n",
    "    \"\"\"\n",
    "    # 1. load objects\n",
    "    (scaler, normalizer, encoder, features, models) = load_objects()\n",
    "    model = models[model_name]\n",
    "    n_features = model.n_features_in_\n",
    "\n",
    "    # 2. read input from user and convert to dataframe\n",
    "    raw_input_df = pd.DataFrame([raw_input_from_user])\n",
    "\n",
    "    # 3. process data / data pipeline\n",
    "    if n_features == 4:\n",
    "        input_data_to_model = data_pipeline_v1(raw_input_df, scaler)\n",
    "    else:\n",
    "        input_data_to_model = data_pipeline_v2(raw_input_df, features[\"engineered\"], normalizer)\n",
    "    \n",
    "    # 4. prediction\n",
    "    prediction = model.predict(input_data_to_model)\n",
    "    prediction_proba = model.predict_proba(input_data_to_model).max() * 100\n",
    "\n",
    "    # 5. interpret output\n",
    "    prediction_str = encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "    return prediction, prediction_proba, prediction_str\n",
    "\n",
    "# (loaded_scaler, loaded_normalizer, loaded_encoder, loaded_features, loaded_models) = load_objects()\n",
    "# raw_user_input_df = pd.DataFrame([raw_user_input])\n",
    "# display(data_pipeline_v1(raw_user_input_df,loaded_scaler ))\n",
    "# display(data_pipeline_v2(raw_user_input_df, loaded_features[\"engineered\"], loaded_normalizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pipeline(raw_user_input, \"SVM\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
